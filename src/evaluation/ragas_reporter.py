"""
Ragas evaluation reporting module.
Generates HTML and JSON reports for ragas evaluation results.
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)

class RagasReporter:
    """Generate reports for ragas evaluation results."""
    
    def __init__(self):
        """Initialize reporter."""
        self.template_dir = Path(__file__).parent / "templates"
        self.template_dir.mkdir(exist_ok=True)
    
    def generate_html_report(self, evaluation_result: Dict[str, Any], 
                           output_path: str) -> str:
        """Generate HTML report from evaluation results."""
        logger.info(f"Generating HTML report: {output_path}")
        
        # Create HTML content
        html_content = self._create_html_content(evaluation_result)
        
        # Write to file
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTML report generated: {output_path}")
        return str(output_path)
    
    def generate_json_report(self, evaluation_result: Dict[str, Any], 
                           output_path: str) -> str:
        """Generate JSON report from evaluation results."""
        logger.info(f"Generating JSON report: {output_path}")
        
        # Prepare report data
        report_data = self._prepare_report_data(evaluation_result)
        
        # Write to file
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"JSON report generated: {output_path}")
        return str(output_path)
    
    def _create_html_content(self, evaluation_result: Dict[str, Any]) -> str:
        """Create HTML content for the report."""
        summary = evaluation_result.get('summary', {})
        metadata = evaluation_result.get('metadata', {})
        results = evaluation_result.get('results', [])
        
        # Calculate additional statistics
        stats = self._calculate_statistics(results)
        
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ragas Evaluation Report</title>
    <style>
        {self._get_css_styles()}
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div class="container">
        <header>
            <h1>üîç Ragas RAG Evaluation Report</h1>
            <p class="subtitle">Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </header>
        
        <div class="summary-section">
            <h2>üìä Executive Summary</h2>
            {self._create_summary_cards(summary, stats)}
        </div>
        
        <div class="metrics-section">
            <h2>üìà Metrics Overview</h2>
            {self._create_metrics_charts(summary)}
        </div>
        
        <div class="details-section">
            <h2>üìã Detailed Results</h2>
            {self._create_results_table(results)}
        </div>
        
        <div class="insights-section">
            <h2>üí° Insights & Recommendations</h2>
            {self._create_insights(summary, stats)}
        </div>
        
        <footer>
            <p>Report generated by Ragas Evaluation Framework</p>
            <p>Evaluator: {metadata.get('evaluator', 'ragas')} | 
               Metrics: {', '.join(metadata.get('metrics_used', []))}</p>
        </footer>
    </div>
    
    <script>
        {self._get_javascript(summary)}
    </script>
</body>
</html>
"""
        return html
    
    def _get_css_styles(self) -> str:
        """Get CSS styles for the HTML report."""
        return """
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f7fa;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .subtitle {
            opacity: 0.9;
            font-size: 1.1rem;
        }
        
        .summary-section, .metrics-section, .details-section, .insights-section {
            background: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: #2d3748;
            margin-bottom: 1.5rem;
            font-size: 1.8rem;
        }
        
        .cards-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-bottom: 2rem;
        }
        
        .card {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 8px;
            text-align: center;
        }
        
        .card.success {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }
        
        .card.warning {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }
        
        .card.danger {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }
        
        .card-value {
            font-size: 2rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
        }
        
        .card-label {
            font-size: 0.9rem;
            opacity: 0.9;
        }
        
        .chart-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin-bottom: 2rem;
        }
        
        .chart-box {
            background: #f8f9fa;
            padding: 1.5rem;
            border-radius: 8px;
            text-align: center;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1rem;
        }
        
        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid #e2e8f0;
        }
        
        th {
            background-color: #f7fafc;
            font-weight: 600;
            color: #2d3748;
        }
        
        tr:hover {
            background-color: #f7fafc;
        }
        
        .score-cell {
            font-weight: bold;
        }
        
        .score-excellent { color: #38a169; }
        .score-good { color: #3182ce; }
        .score-fair { color: #d69e2e; }
        .score-poor { color: #e53e3e; }
        
        .insights-list {
            list-style: none;
            padding: 0;
        }
        
        .insights-list li {
            background: #f7fafc;
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 6px;
            border-left: 4px solid #667eea;
        }
        
        .recommendation {
            background: #e6fffa;
            border-left-color: #38b2ac;
        }
        
        .warning {
            background: #fffbeb;
            border-left-color: #d69e2e;
        }
        
        footer {
            text-align: center;
            padding: 2rem;
            color: #718096;
            font-size: 0.9rem;
        }
        
        @media (max-width: 768px) {
            .chart-container {
                grid-template-columns: 1fr;
            }
            
            .cards-container {
                grid-template-columns: 1fr;
            }
        }
        """
    
    def _create_summary_cards(self, summary: Dict[str, Any], stats: Dict[str, Any]) -> str:
        """Create summary cards HTML."""
        total_examples = summary.get('total_examples', 0)
        avg_score = summary.get('average_overall_score', 0.0)
        success_rate = summary.get('success_rate', 0.0)
        high_performers = summary.get('scores_above_0.8', 0)
        
        return f"""
        <div class="cards-container">
            <div class="card success">
                <div class="card-value">{total_examples}</div>
                <div class="card-label">Total Examples</div>
            </div>
            <div class="card {self._get_score_class(avg_score)}">
                <div class="card-value">{avg_score:.3f}</div>
                <div class="card-label">Average Score</div>
            </div>
            <div class="card success">
                <div class="card-value">{success_rate:.1%}</div>
                <div class="card-label">Success Rate</div>
            </div>
            <div class="card warning">
                <div class="card-value">{high_performers}</div>
                <div class="card-label">High Performers (>0.8)</div>
            </div>
        </div>
        """
    
    def _create_metrics_charts(self, summary: Dict[str, Any]) -> str:
        """Create metrics charts HTML."""
        return """
        <div class="chart-container">
            <div class="chart-box">
                <h3>Metric Scores</h3>
                <canvas id="metricsChart" width="400" height="200"></canvas>
            </div>
            <div class="chart-box">
                <h3>Score Distribution</h3>
                <canvas id="distributionChart" width="400" height="200"></canvas>
            </div>
        </div>
        """
    
    def _create_results_table(self, results: List[Any]) -> str:
        """Create results table HTML."""
        if not results:
            return "<p>No results to display.</p>"
        
        # Take first 20 results for display
        display_results = results[:20]
        
        table_rows = ""
        for i, result in enumerate(display_results, 1):
            # Handle both dataclass and dict formats
            if hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result
                
            faithfulness = result_dict.get('faithfulness', 0.0)
            answer_relevancy = result_dict.get('answer_relevancy', 0.0)
            context_precision = result_dict.get('context_precision', 0.0)
            context_recall = result_dict.get('context_recall', 0.0)
            overall = result_dict.get('overall_score', 0.0)
            
            question = ""
            if 'details' in result_dict and isinstance(result_dict['details'], dict):
                question = result_dict['details'].get('question', f'Question {i}')
            else:
                question = f'Question {i}'
            
            question = question[:100] + "..." if len(question) > 100 else question
            
            table_rows += f"""
            <tr>
                <td>{i}</td>
                <td>{question}</td>
                <td class="score-cell {self._get_score_class(faithfulness)}">{faithfulness:.3f}</td>
                <td class="score-cell {self._get_score_class(answer_relevancy)}">{answer_relevancy:.3f}</td>
                <td class="score-cell {self._get_score_class(context_precision)}">{context_precision:.3f}</td>
                <td class="score-cell {self._get_score_class(context_recall)}">{context_recall:.3f}</td>
                <td class="score-cell {self._get_score_class(overall)}">{overall:.3f}</td>
            </tr>
            """
        
        total_results = len(results)
        showing_message = f"<p><em>Showing first 20 of {total_results} results</em></p>" if total_results > 20 else ""
        
        return f"""
        {showing_message}
        <table>
            <thead>
                <tr>
                    <th>#</th>
                    <th>Question</th>
                    <th>Faithfulness</th>
                    <th>Answer Relevancy</th>
                    <th>Context Precision</th>
                    <th>Context Recall</th>
                    <th>Overall Score</th>
                </tr>
            </thead>
            <tbody>
                {table_rows}
            </tbody>
        </table>
        """
    
    def _create_insights(self, summary: Dict[str, Any], stats: Dict[str, Any]) -> str:
        """Create insights and recommendations."""
        insights = []
        
        avg_score = summary.get('average_overall_score', 0.0)
        high_performers = summary.get('scores_above_0.8', 0)
        low_performers = summary.get('scores_below_0.5', 0)
        total_examples = summary.get('total_examples', 1)
        
        # Performance insights
        if avg_score >= 0.8:
            insights.append("‚úÖ <strong>Excellent Performance:</strong> Your RAG system is performing exceptionally well with high accuracy and relevance.")
        elif avg_score >= 0.6:
            insights.append("üëç <strong>Good Performance:</strong> Your RAG system shows solid performance with room for optimization.")
        else:
            insights.append("‚ö†Ô∏è <strong>Performance Needs Improvement:</strong> Consider optimizing your retrieval strategy and prompt engineering.")
        
        # Specific metric insights
        faithfulness_avg = summary.get('average_faithfulness', 0.0)
        if faithfulness_avg < 0.6:
            insights.append("""
            <div class="warning">
            <strong>Faithfulness Concern:</strong> Low faithfulness scores suggest the model may be generating information not supported by the retrieved contexts. 
            Consider improving context quality or adjusting generation parameters.
            </div>
            """)
        
        answer_relevancy_avg = summary.get('average_answer_relevancy', 0.0)
        if answer_relevancy_avg < 0.6:
            insights.append("""
            <div class="warning">
            <strong>Relevancy Issue:</strong> Answers may not be sufficiently relevant to the questions. 
            Review your retrieval strategy and ensure the most relevant documents are being selected.
            </div>
            """)
        
        # Recommendations
        recommendations = []
        
        if low_performers > total_examples * 0.2:
            recommendations.append("""
            <div class="recommendation">
            <strong>Recommendation:</strong> More than 20% of examples scored below 0.5. 
            Consider fine-tuning your embedding model or expanding your knowledge base.
            </div>
            """)
        
        if high_performers < total_examples * 0.3:
            recommendations.append("""
            <div class="recommendation">
            <strong>Recommendation:</strong> Less than 30% of examples achieved high scores. 
            Consider implementing query expansion or improving document preprocessing.
            </div>
            """)
        
        recommendations.append("""
        <div class="recommendation">
        <strong>Next Steps:</strong> 
        1. Analyze low-performing examples to identify common patterns
        2. Consider A/B testing different retrieval strategies
        3. Monitor performance over time with regular evaluations
        </div>
        """)
        
        all_insights = insights + recommendations
        insights_html = "\n".join(f"<li>{insight}</li>" for insight in all_insights)
        
        return f'<ul class="insights-list">{insights_html}</ul>'
    
    def _get_score_class(self, score: float) -> str:
        """Get CSS class for score coloring."""
        if score >= 0.8:
            return "score-excellent"
        elif score >= 0.6:
            return "score-good"
        elif score >= 0.4:
            return "score-fair"
        else:
            return "score-poor"
    
    def _get_javascript(self, summary: Dict[str, Any]) -> str:
        """Get JavaScript for charts."""
        # Extract metric data
        metrics = {
            'Faithfulness': summary.get('average_faithfulness', 0.0),
            'Answer Relevancy': summary.get('average_answer_relevancy', 0.0),
            'Context Precision': summary.get('average_context_precision', 0.0),
            'Context Recall': summary.get('average_context_recall', 0.0)
        }
        
        # Score distribution (example data - would need actual distribution)
        distribution = {
            'Excellent (>0.8)': summary.get('scores_above_0.8', 0),
            'Good (0.6-0.8)': max(0, summary.get('total_examples', 0) - summary.get('scores_above_0.8', 0) - summary.get('scores_below_0.5', 0)),
            'Poor (<0.5)': summary.get('scores_below_0.5', 0)
        }
        
        return f"""
        // Metrics Chart
        const metricsCtx = document.getElementById('metricsChart').getContext('2d');
        new Chart(metricsCtx, {{
            type: 'radar',
            data: {{
                labels: {list(metrics.keys())},
                datasets: [{{
                    label: 'Score',
                    data: {list(metrics.values())},
                    backgroundColor: 'rgba(102, 126, 234, 0.2)',
                    borderColor: 'rgba(102, 126, 234, 1)',
                    borderWidth: 2
                }}]
            }},
            options: {{
                responsive: true,
                scales: {{
                    r: {{
                        beginAtZero: true,
                        max: 1.0
                    }}
                }}
            }}
        }});
        
        // Distribution Chart
        const distributionCtx = document.getElementById('distributionChart').getContext('2d');
        new Chart(distributionCtx, {{
            type: 'doughnut',
            data: {{
                labels: {list(distribution.keys())},
                datasets: [{{
                    data: {list(distribution.values())},
                    backgroundColor: [
                        'rgba(56, 161, 105, 0.8)',
                        'rgba(49, 130, 206, 0.8)',
                        'rgba(229, 62, 62, 0.8)'
                    ]
                }}]
            }},
            options: {{
                responsive: true,
                plugins: {{
                    legend: {{
                        position: 'bottom'
                    }}
                }}
            }}
        }});
        """
    
    def _calculate_statistics(self, results: List[Any]) -> Dict[str, Any]:
        """Calculate additional statistics from results."""
        if not results:
            return {}
        
        # Convert results to list of dicts
        result_dicts = []
        for result in results:
            if hasattr(result, '__dict__'):
                result_dicts.append(result.__dict__)
            else:
                result_dicts.append(result)
        
        # Calculate distribution
        score_ranges = {'excellent': 0, 'good': 0, 'fair': 0, 'poor': 0}
        
        for result in result_dicts:
            score = result.get('overall_score', 0.0)
            if score >= 0.8:
                score_ranges['excellent'] += 1
            elif score >= 0.6:
                score_ranges['good'] += 1
            elif score >= 0.4:
                score_ranges['fair'] += 1
            else:
                score_ranges['poor'] += 1
        
        return {
            'score_distribution': score_ranges,
            'total_evaluated': len(results)
        }
    
    def _prepare_report_data(self, evaluation_result: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for JSON report."""
        return {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'report_type': 'ragas_evaluation',
                'version': '1.0'
            },
            'evaluation_summary': evaluation_result.get('summary', {}),
            'evaluation_metadata': evaluation_result.get('metadata', {}),
            'detailed_results': [
                result.__dict__ if hasattr(result, '__dict__') else result 
                for result in evaluation_result.get('results', [])
            ],
            'insights': self._generate_json_insights(evaluation_result.get('summary', {}))
        }
    
    def _generate_json_insights(self, summary: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate insights for JSON report."""
        insights = []
        
        avg_score = summary.get('average_overall_score', 0.0)
        
        if avg_score >= 0.8:
            insights.append({
                'type': 'success',
                'title': 'Excellent Performance',
                'description': 'RAG system is performing exceptionally well',
                'priority': 'info'
            })
        elif avg_score < 0.5:
            insights.append({
                'type': 'warning',
                'title': 'Performance Needs Improvement',
                'description': 'Consider optimizing retrieval strategy and prompt engineering',
                'priority': 'high'
            })
        
        return insights

if __name__ == "__main__":
    # Test the reporter
    logging.basicConfig(level=logging.INFO)
    
    # Mock evaluation result for testing
    mock_result = {
        'summary': {
            'total_examples': 10,
            'average_overall_score': 0.75,
            'success_rate': 0.9,
            'scores_above_0.8': 4,
            'scores_below_0.5': 1,
            'average_faithfulness': 0.8,
            'average_answer_relevancy': 0.7,
            'average_context_precision': 0.75,
            'average_context_recall': 0.7
        },
        'metadata': {
            'evaluator': 'ragas',
            'metrics_used': ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']
        },
        'results': [
            {
                'faithfulness': 0.8,
                'answer_relevancy': 0.7,
                'context_precision': 0.75,
                'context_recall': 0.7,
                'overall_score': 0.75,
                'details': {'question': 'Test question 1'}
            }
        ]
    }
    
    reporter = RagasReporter()
    
    # Test HTML report
    html_path = "test_report.html"
    reporter.generate_html_report(mock_result, html_path)
    print(f"Test HTML report generated: {html_path}")
    
    # Test JSON report  
    json_path = "test_report.json"
    reporter.generate_json_report(mock_result, json_path)
    print(f"Test JSON report generated: {json_path}")